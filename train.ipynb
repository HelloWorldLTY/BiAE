{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scprep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import graphtools as gt\n",
    "import datetime\n",
    "import scanpy as sc\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import loompy as lp\n",
    "import umap.umap_ as umap\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as Data  #Data是用来批训练的模块\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(999)\n",
    "torch.manual_seed(999)\n",
    "torch.cuda.manual_seed_all(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new = sc.read_h5ad('PBMC10_5.h5ad') # Prepared dataset with two results from different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Matching(data1, data2, label_list):\n",
    "  celltype1 = label_list[0]\n",
    "  celltype2 = label_list[1]\n",
    "\n",
    "  id_list1 = [i for i in range(len(data1))]\n",
    "  id_list2 = [i for i in range(len(data2))]\n",
    "\n",
    "  result_pair = []\n",
    "\n",
    "  while id_list2 != []:\n",
    "    item = id_list2[0]\n",
    "    temp = [i for i in range(len(id_list1)) if celltype1[i]==celltype2[item]]\n",
    "    k = np.random.choice(temp)\n",
    "    result_pair.append((k, item))\n",
    "    id_list2.remove(item)\n",
    "\n",
    "  return [result_pair,id_list1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.obsm['protein_expression']\n",
    "\n",
    "pro_test = adata_new.obsm['protein_expression'][['CD3','CD4','CD8a','CD14','CD16','CD19']]\n",
    "\n",
    "pro_test\n",
    "\n",
    "adata_0 = adata_new[:,['CD3D','CD8B','CD8A','CYBB','CHL1','CCL5']]\n",
    "\n",
    "adata_0.obsm['protein_expression'] = pro_test\n",
    "\n",
    "adata_0.obs_names_make_unique()\n",
    "\n",
    "adata_0.write_h5ad('correlationfind_pbmc.h5ad')\n",
    "\n",
    "adata_1 = adata_new[0:2000,:]\n",
    "\n",
    "adata_2 = adata_new[2000:5000,:]\n",
    "\n",
    "train_data_b1 = adata_1.X \n",
    "train_label_b1 = adata_1.obsm['protein_expression']\n",
    "\n",
    "train_data_b2 = adata_2.X \n",
    "train_label_b2 = adata_2.obsm['protein_expression']\n",
    "\n",
    "adata_0 = adata_1.concatenate(adata_2, batch_categories=['batch1','batch2'])\n",
    "\n",
    "label = [np.array(adata_1.obs['celltype']), np.array(adata_2.obs['celltype'])]\n",
    "\n",
    "\n",
    "pair_info, res_id = KNN_Matching(adata_1.X, adata_2.X, label)\n",
    "\n",
    "\n",
    "rna_train_index = [i[0] for i in pair_info]\n",
    "pro_train_index = [i[1] for i in pair_info]\n",
    "\n",
    "train_data = adata_1.X[rna_train_index]\n",
    "train_label = adata_2.obsm['protein_expression'].values[pro_train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mish(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self,x):\n",
    "    return x*torch.tanh(F.softplus(x))\n",
    "\n",
    "# NN model\n",
    "# use supervisied learning method\n",
    "# target: transfer rna data into protein data\n",
    "\n",
    "class generator_r2p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator_r2p, self).__init__()\n",
    "        self.relu_l = nn.ReLU(True)\n",
    "        self.gen = nn.Sequential(\n",
    "\n",
    "            nn.Linear(2000, 1024),  \n",
    "            nn.BatchNorm1d(1024),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(1024, 512),  \n",
    "            nn.BatchNorm1d(512),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(512, 14)\n",
    "           \n",
    "        )\n",
    "\n",
    "        self.lin = nn.Linear(2000, 14)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ge = self.gen(x)\n",
    "        \n",
    "        return ge\n",
    "\n",
    "\n",
    "\n",
    "class generator_p2r(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator_p2r, self).__init__()\n",
    "        self.relu_l = nn.ReLU(True)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(14,128),  \n",
    "            nn.BatchNorm1d(128),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(128, 256),  \n",
    "            nn.BatchNorm1d(256),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(256, 512),  \n",
    "            nn.BatchNorm1d(512),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(512, 1024),  \n",
    "            nn.BatchNorm1d(1024),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(1024, 2000),  \n",
    "           \n",
    "        )\n",
    "\n",
    "        self.lin = nn.Linear(14,2000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_l(self.gen(x) + self.lin(x))\n",
    "        return x\n",
    "\n",
    "class discriminator_r2p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator_r2p, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(14,6),  \n",
    "            nn.BatchNorm1d(6),\n",
    "            Mish(),\n",
    "            nn.Linear(6,1),  \n",
    "            nn.ReLU(True)\n",
    "           \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.disc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class discriminator_p2r(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator_p2r, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(4000,2048),  \n",
    "            # nn.BatchNorm1d(2048),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(2048, 1024),  \n",
    "            # nn.BatchNorm1d(1024),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(1024, 512),  \n",
    "            # nn.BatchNorm1d(512),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(512, 256),  \n",
    "            # nn.BatchNorm1d(256),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(256, 128),  \n",
    "            # nn.BatchNorm1d(128),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(128, 64),  \n",
    "            # nn.BatchNorm1d(64),\n",
    "            Mish(),\n",
    "\n",
    "            nn.Linear(64, 1)  \n",
    "            # nn.ReLU(True)\n",
    "           \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.disc(x)\n",
    "        return x\n",
    "\n",
    "EPOCH1 = 100 #old value : 200\n",
    "MAX_ITER = train_data.shape[0]\n",
    "batch = 32\n",
    "BATCH = 32\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "lambda_1 = 1/10\n",
    "\n",
    "Encoder = generator_r2p()\n",
    "Decoder = generator_p2r()\n",
    "# use GPU\n",
    "if torch.cuda.is_available():\n",
    "  Encoder = Encoder.cuda()\n",
    "  Decoder = Decoder.cuda()\n",
    "Encoder.train()\n",
    "Decoder.train()\n",
    "\n",
    "criterion = nn.SmoothL1Loss() \n",
    "if torch.cuda.is_available():\n",
    "  criterion = criterion.cuda()\n",
    "encoder_optimizer = torch.optim.Adam(Encoder.parameters(), lr=0.00001)\n",
    "decoder_optimizer = torch.optim.Adam(Decoder.parameters(), lr=0.00001)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(EPOCH1):\n",
    "  print(epoch)\n",
    "  print(\"###########################Encoder Part#######################\")\n",
    "  for time in range(0,MAX_ITER,BATCH):\n",
    "    train = torch.FloatTensor(train_data[time:time+BATCH,:]).cuda()\n",
    "    label = torch.FloatTensor(train_label[time:time+BATCH,:]).cuda()\n",
    "\n",
    "    #train encoder\n",
    "    output = Encoder(train)\n",
    "\n",
    "    err_r2p = criterion(output, label)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    err_r2p.backward()\n",
    "    encoder_optimizer.step()\n",
    "\n",
    "    if(time%100==0):\n",
    "      print('encoder part loss', err_r2p)\n",
    "  print(\"###########################Decoder Part#######################\")\n",
    "\n",
    "  for time in range(0,MAX_ITER,BATCH):\n",
    "    train = torch.FloatTensor(train_data[time:time+BATCH,:]).cuda()\n",
    "    label = torch.FloatTensor(train_label[time:time+BATCH,:]).cuda()\n",
    "\n",
    "    #train encoder\n",
    "    output = Decoder(label)\n",
    "\n",
    "    err_p2r = criterion(output, train)\n",
    "\n",
    "    decoder_optimizer.zero_grad()\n",
    "    err_p2r.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    if(time%100==0):\n",
    "      print('decoder part loss', err_p2r)\n",
    "  print(\"###########################Construct Part#######################\")\n",
    "  for time in range(0,MAX_ITER,BATCH):\n",
    "    train = torch.FloatTensor(train_data[time:time+BATCH,:]).cuda()\n",
    "    label = torch.FloatTensor(train_label[time:time+BATCH,:]).cuda()\n",
    "\n",
    "    #train encoder\n",
    "    output = Decoder(Encoder(train))\n",
    "\n",
    "    err_cons = criterion(output, train)\n",
    "\n",
    "    decoder_optimizer.zero_grad()\n",
    "    err_cons.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    if(time%100==0):\n",
    "      print('construct part loss', err_cons)\n",
    "  print(\"###########################Reconstruct Part#######################\")\n",
    "  for time in range(0,MAX_ITER,BATCH):\n",
    "    train = torch.FloatTensor(train_data[time:time+BATCH,:]).cuda()\n",
    "    label = torch.FloatTensor(train_label[time:time+BATCH,:]).cuda()\n",
    "\n",
    "    #train encoder\n",
    "    output = Encoder(Decoder(label))\n",
    "\n",
    "    err_cons = criterion(output, label)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    err_cons.backward()\n",
    "    encoder_optimizer.step()\n",
    "\n",
    "    if(time%100==0):\n",
    "      print('reconstruct part loss', err_cons)\n",
    "\n",
    "print(\"#######################fubusged pre train##########################\")\n",
    "\n",
    "# generate test data\n",
    "def get_train_result(G,testd):\n",
    "  G.eval()\n",
    "  test_data1 = torch.FloatTensor(testd).cuda()\n",
    "  test_list = G(test_data1).detach().cpu().numpy() \n",
    "  return test_list \n",
    "\n",
    "rna = get_train_result(Decoder, train_label_b2.values)\n",
    "\n",
    "rna_t = np.vstack([train_data_b1,rna])\n",
    "\n",
    "adata_0.X = rna_t\n",
    "sc.tl.pca(adata_0, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umap = umap.UMAP().fit_transform(adata_0.obsm['X_pca'])\n",
    "\n",
    "scprep.plot.scatter2d(data_umap, c=adata_new.obs['celltype'][0:5000], figsize=(12,8), cmap=\"Spectral\",\n",
    "                      ticks=False, label_prefix=\"UMAP\", s = 20, title = 'Celltype Specific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(data_umap, c=adata_0.obs['batch'], figsize=(12,8), cmap=\"Spectral\",\n",
    "                      ticks=False, label_prefix=\"UMAP\", s = 20, title = 'Celltype Specific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = get_train_result(Encoder, train_data_b1)\n",
    "\n",
    "pro_t = np.vstack([pro, train_label_b2.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_umap = umap.UMAP().fit_transform(pro_t)\n",
    "\n",
    "scprep.plot.scatter2d(data_umap, c=adata_new.obs['celltype'][0:5000], figsize=(12,8), cmap=\"Spectral\",\n",
    "                      ticks=False, label_prefix=\"UMAP\", s = 20, title = 'Celltype Specific')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(data_umap, c=adata_0.obs['batch'], figsize=(12,8), cmap=\"Spectral\",\n",
    "                      ticks=False, label_prefix=\"UMAP\", s = 20, title = 'Celltype Specific')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store output files as adata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new0 = adata_new[0:5000]\n",
    "\n",
    "adata_new0.X = rna_t\n",
    "\n",
    "adata_new0.obsm['protein_expression'] = pro_t\n",
    "\n",
    "adata_new0.write_h5ad('Finished_Running_BIDpart1.h5ad')"
   ]
  }
 ]
}